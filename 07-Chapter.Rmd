# Statistical Spatial Interpolation

**"Compassion isn't about being nice, it's about reducing suffering for others."** -- April Wensel

## Introduction

Observations of the natural world are made at specific locations in space (and time). But we often want estimates of the observed values everywhere. This is the case when the observations are taken from a continuous field (surface). Data observed or measured at locations across a continuous field are called geostatistical data. Examples: concentrations of heavy metals across a farm field, surface air pressures measured by barometers at cities across the country, minimum air temperature values across the city on a clear, calm night.

Local averaging or spline functions are typically used for interpolation. If it is 20C here and 30C ten miles to the south, then it is 25C five miles to the south. That is a reasonable first-order assumption. But, these methods do not (1) take into account spatial autocorrelation and (2) do not estimate uncertainty about the interpolated values.

Kriging is statistical interpolation (usually spatial). It is the centerpiece of what is called 'geostatistics.' The resulting surface (kriged surface) is composed of three components. (1) Spatial trend: an increase or decrease in the values that depends on direction or a covariate (co-kriging); (2) Local spatial autocorrelation. (3) Random variation. Together the three components provide a model that is used to estimate values at any point in a specified domain.

Geostatistics is used to (1) quantify spatial correlation, (2) predict values at specific locations, (3) provide an estimate of uncertainty on the predicted values, and (4) simulation.

As we've done with areal averaged data and point pattern data (Moran's I, Ripley's K, etc), we begin with understanding how to quantify spatial autocorrelation. In geostatistics, this involves the covariance function, the correlogram function, and the variogram.

### Continuity and stationarity

* Statistical interpolation assumes the observed values are spatially homogeneous. This implies stationarity and continuity.
* Stationarity implies that the average difference in values between pairs of observations separated by a given distance (lag) is constant across the domain. 
* Continuity implies that the spatial autocorrelation depends only on the lag (and orientation) between observations. That is; spatial autocorrelation is independent of location.
* Stationarity and continuity allow different parts of the domain to be treated as "independent" samples. 
* Spatial autocorrelation can be described by a single parametric function. 

Stationarity can be weak or intrinsic. Both assume the average of the difference in values at observations separated by a lag distance $h$ is zero. That is, E$[z_i - z_j]$ = 0, where location $i$ and location $j$ are a (lag) distance $h$ apart. This implies that the interpolated surface $Z(s)$ is a random function with a constant mean ($m$) and a residual ($\varepsilon$).
$$
Z(s) = m + \varepsilon(s).
$$
The expected value (average across all values) in the domain is $m$.

Weak stationarity assumes the covariance is a function of the lag distance $h$.
$$
\hbox{cov}(z_i, z_j) = \hbox{cov}(h)
$$
where cov($h$) is called the covariogram.

Intrinsic stationarity assumes the variance of the difference in values is a function of the lag: 
$$
\hbox{var}(z_i - z_j) = \gamma(h),
$$
where $\gamma(h)$ is called the variogram. This means that the variance of $Z$ is constant and that spatial correlation is independent of location.

These assumptions are needed to get started with statistical interpolation.

### Covariogram and correlogram

Our interest will be on a parametric model for the variogram $\gamma(h)$. But to help understand the variogram, let us first consider the covariogram. 

To make things simple but with no loss in generality, we start with a 4 x 6 map of surface air temperatures in degrees C.

  21  21  20  19  18  19 
  
  26  25  26  27  29  28 
  
  32  33  34  35  30  28   
  
  34  35  35  36  32  31   

Put the values into a data vector and determine the mean and variance.
```{r chapter7}
temps <- c(21, 21, 20, 19, 18, 19, 
           26, 25, 26, 27, 29, 28, 
           32, 33, 34, 35, 30, 28, 
           34, 35, 35, 36, 32, 31)
mean(temps); var(temps)
```

To start, we focus only on the north-south direction. To compute the sample covariance function we first compute the covariance between the observed values one distance unit apart.

Mathematically
$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum (z_i - Z)(z_j - Z)
$$
where $|N(1)|$ is the number of distinct observation pairs with a distance separation of one unit in the north-south direction and where $Z$ is the average over all observations. Here we let zero in the cov(0, 1) refer to the direction and the one to the distance of one unit apart. Here $|N(1)|$ = 18.

The equation for the covariance can be simplified to:
$$
\hbox{cov}(0, 1) = 1/|N(1)| \sum z_i z_j - m_{-1} m_{+1}
$$
where $m_{-1}$ is the average temperature over all rows except the first (northern most) and $m_{+1}$ is the average temperature over all rows except the last (southern most).

To simplify the notation we re-index the grid of temperatures using lexicographic (reading) order.

 1   2   3   4   5   6
 
 7   8   9   10  11  12 
 
 13  14  15  16  17  18  
  
 19  20  21  22  23  24 

Then
```{r}
mp1 <- mean(temps[1:18])
mm1 <- mean(temps[7:24])
cc <- sum(temps[1:18] * temps[7:24])/18
cc - mm1 * mp1
```

Or more generally
```{r}
N <- 18
k <- 1:N
1/N * sum(temps[k] * temps[k + 6]) - mean(temps[k]) * mean(temps[k + 6])
```

The covariance has units of the field variable squared (here $^\circ C^2$).

We also have observation pairs two units of distance apart. So we compute the cov(0, 2) in a similar way. 
$$
\hbox{cov}(0, 2) = 1/|N(2)| \sum z_i z_j - m_{-2} m_{+2}
$$
where $m_{-2}$ is the average temperature over all rows except the first two and $m_{+2}$ is the average temperature over all rows except the last two. $|N(2)|$ is the number of pairs two units apart.
```{r}
N <- 12
k <- 1:N
1/N * sum(temps[k] * temps[k + 12]) - mean(temps[k]) * mean(temps[k + 12])
```

Similarly we have observation pairs three units apart so we compute cov(0, 3) as
$$
\hbox{cov}(0, 3) = 1/|N(3)| \sum z_i z_j - m_{-3} m_{+3}
$$
```{r}
N <- 6
k <- 1:N
1/N * sum(temps[k] * temps[k + 18]) - mean(temps[k]) * mean(temps[k + 18])
```

There are no observation pairs four units apart in the north-south direction so we are finished. The covariogram is a plot of the covariance values as a function of lagged distance. Let h be the lagged distance, then

h      |  cov(h)  
-------|--------  
(0, 1) |  15  
(0, 2) |   3  
(0, 3) |   1  

It is convenient to have a measure of co-variability that is dimensionless. This is obtained by dividing the covariance at lagged distance $h$ by the covariance at lag zero. This is the correlogram. Values of the correlogram range from 0 to +1.

### Variogram

The covariogram is a decreasing function of lag. The variogram is the multiplicative inverse of the covariogram. 

Mathematically: var($z_i - z_j$) for locations i and j. The semivariogram is 1/2 the variogram. If location i is near location j, the difference in the values will be small and so too will the variance of their differences, in general. If location i is far from location j, the difference in values will be large and so too will the variance of their differences.

In practice we have a set of observations and we compute a variogram. We call this the sample (or empirical) variogram. Let $t_i = (x_i, y_i)$  be the ith location and $h_{i,j} = t_j - t_i$ be the vector connecting location $t_i$ with location $t_j$. Then the sample variogram is defined as
$$
\gamma(h) = \frac{1}{2N(h)} \sum^{N(h)} (z_i - z_j)^2
$$
where $N(h)$ is the number of observation pairs a distance of $h$ units apart.

The variogram assumes intrinsic stationarity so the values need to be detrended first.

The sample variogram is characterized by a set of points the values of which generally increase as $h$ increases before leveling off (reaching a plateau).

### Terminology

Let's begin with a plot with labels. Make sure the {geoR} package is installed. The code is done using the base graphics commands and the plot method from the package.
```{r}
library(geoR)
plot(variog(s100, max.dist = 1), 
     xlab = "Lagged Distance (h)",
     ylab = expression(paste(gamma,"(h)")), 
     las = 1, pch = 16)
abline(h = 0)
abline(h = .15, col = "red")
arrows(0, 0, x1 = 0, y1 = .15, length = .1)
text(0, y = .05, labels = "nugget", pos=4)
abline(h = .9, col = "red")
arrows(0, .15, x1 = 0, y1 = .9, length = .1)
text(0, y = .8, labels="sill (partial sill)", pos=4)
abline(v = .6, col = "red")
arrows(0, 1, x1 = .6, y1 = 1, length = .1)
text(.4, y = 1.04, labels = "range")
```

* Lag (lag distance): Relative distance between observation locations.
* Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag. The nugget is the variation in the values at the observation locations without regard to spatial variation. Related to the observation (or measurement) precision.
* Sill: The height of the variogram at which the values are uncorrelated.
* Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage.
* Range: The distance beyond which the values are uncorrelated. The range is indicated on the empirical variogram as the position along the horizontal axis where values of the variogram reach a constant height.

Additional terms.
* Isotropy: The condition in which spatial correlation is the same in all directions.
* Anisotropy: (an-I-so-trop-y) spatial correlation is stronger or more persistent in some directions.
* Directional variogram: Distance and direction are important in characterizing the spatial correlations. Otherwise the variogram is called omni-directional.
* Azimuth ($\theta$): Defines the direction of the variogram in degrees.The azimuth is measured clockwise from north.
* Lag spacing: The distance between successive lags is called the lag spacing or lag increment.
* Lag tolerance: The distance allowable for observational pairs at a specified lag. With arbitrary observation locations there will be no observations exactly a lag distance from any observation. Lag tolerance provides a range of distances to be used for computing values of the variogram at a specified lag.

### Variogram Models

Computing the sample variogram is the first step in modeling geostatistical data. The next step is to fit a model to these variogram estimates. We replace a scatter of points with a statistical model. 

The model is important since the sample variogram is made only at specified lag distances (with specified lag tolerance and azimuth). We need a continuous function that varies smoothly across all lags.

Variogram models come from different families. The decision includes (1) choosing the model family and (2) determining the parameters: nugget, sill, and range.

The exponential model family reaches the sill asymptotically. The range (a) is defined as the lag distance at which gamma reaches 95% of the sill.
```{r}
c0 <- .1
c1 <- 2.1
a <- 1.3
curve(c0 + c1*(1 - exp(-3*x/a)), 
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

The spherical model family is piece wise reaching the sill at x = 1 (here).
```{r}
curve(c0 + c1*(3*x/2 - x^3/2),
      from = .01, to = 1,
      xlab = "h",
      ylab = expression(paste(hat(gamma), "(h)")), 
      las = 1)
```

The Gaussian model family is sigmoidal. It is used when the data exhibit strong correlations at the shortest lag distances.  The inflection point of the model occurs at $\sqrt{a/6}$.
```{r}
curve(c0 + c1*(1 - exp(-3*x^2/a^2)),
      from = .01, to = 3, 
      xlab = "h", 
      ylab = expression(paste(hat(gamma), "(h)")),
      las = 1)
```

Other families include

* Linear models: $\hat \gamma(h)$ = c0 + b * h.
* Power models:  $\hat \gamma(h)$ = c0 + b * h$^\lambda$.

These models have no sill.

Choosing a variogram family is largely done by eyeballing the shape of the sample variogram. Then, given a sample variogram computed from a set of spatial observations and a choice of family, the parameters of the variogram model are determined by a weighted least-squares (WLS) algorithm. There are other ways to determine the parameters including by eye, and by the method of maximum likelihoods, but WLS is less erratic than other methods and it requires fewer assumptions about the distribution of the data.

### Kriging

The final step is kriging. Kriging interpolates the observed data using the variogram model. It was developed by a South African miner (D.G. Krige) as a way to improve estimates of where ore reserves might be located. Extraction costs are reduced substantially if good predictions can be made of where the ore resides given samples taken across the mine.

A kriged estimate is a weighted average of the observations where the weights are based on the variogram model. The kriged estimates are optimal in the sense that they minimize the error variance. The type of kriging depends on the characteristics of the observations and the purpose of interpolation.

* Simple kriging assumes a known constant mean for the domain.  
* Ordinary kriging assumes an unknown constant mean.  
* Universal kriging assumes an unknown linear or nonlinear trend in the mean.  

The steps are:

1. Examine the observations for trends and isotropy.
2. Compute an empirical variogram.
3. Fit a variogram model to the empirical variogram.
4. Create an interpolated surface using kriging.

## Working with {geoR} data

The {geoR} package has functions for doing geostatistics. There are others but it was one of the first and it is useful for learning how things work. 

Suppose we have the following set of observations (`zobs`) at locations (`sx`, `sy`).
```{r}
sx <- c(1.1, 3.2, 2.1, 4.9, 5.5, 7, 7.8, 9, 2.3, 6.9)
sy <- c(3, 3.5, 6, 1.5, 5.5, 3.2, 1, 4.5, 1, 7)
zobs <- c(-0.6117, -2.4232, -0.42, -0.2522, -2.0362, 0.9814, 1.842,
         0.1723, -0.0811, -0.3896)
```

Create a data frame and plot the observed values at the locations using the `geom_text()` function.
```{r}
df <- data.frame(sx, sy, zobs)

library(ggplot2)

ggplot(df, aes(x = sx, y = sy, label = zobs)) +
  geom_text() +
  theme_minimal()
```

Lag distance (distance between locations) is the independent variable in the variogram function. We get all pairwise distances by applying the `dist()` function to a matrix of spatial coordinates.
```{r}
dist(cbind(sx, sy))
max(dist(cbind(sx, sy)))
min(dist(cbind(sx, sy)))
```

The function computes a pairwise distance matrix. The distance between the first and second observation is 2.16 units and so on. The largest lag distance is 8.04 units and the smallest lag distance is 2.05 units.

The functions in the {geoR} package work with objects of class `geodata`. Thus we first need to convert the data frame of observations and locations into a `geodata` object.

This is done with the `as.geodata()` function. The default for the function is to assume that the first two columns of the data frame contain the coordinates and the third column contains the observed data values. This is the way we constructed the data frame so we can use the defaults.
```{r}
library(geoR)

gdf <- as.geodata(df)
str(gdf)
```

An object of class `geodata` contains two lists: the coordinates of the locations and the observed values as `data`. It may contain other elements like coordinate boundaries but unlike working with `ppp` objects with functions from the {spatstat} package a boundary defining the domain is not required.

For an arbitrary data frame we can specify where the coordinate and data columns by column numbers. For example, if the location coordinates are in columns five and six and the observed values are in column eight then use `coords.col = c(5, 6)` and `data.col = 8`.

With the class set to `geodata` methods like `summary` and `plot` provide attribute and spatial information about the observations. For example the `summary()` method outputs the number of observations, a summary of the coordinates and a summary of the observed values.
```{r}
summary(gdf)
```

We access the coordinates and the data using the `$` operator.
```{r}
gdf$coords
gdf$data
```

The `plot()` method produces a four panel plot.
```{r}
plot(gdf)
```

The upper left panel is a map with the locations of the observations given by colors and symbols according to their values. Green triangles have the smallest values and red crosses have the largest values. We can see an upward trend in values from northwest to southeast.

This trend is decomposed in the upper right and lower left panels. The upper-right panel shows the north-south (Y Coord) coordinate plotted against the data values. As the data values increase to the right the north-south coordinate decreases (smaller data values are in the north). The lower-left panel shows the data plotted against the east-west (X Coord). Moving from west to east we see the data values tend to increase.

The lower right panel is a non-spatial distribution of the data values shown with a histogram, and density curve, and a rug plot.

To plot the trend in the east-west direction, type
```{r}
ggplot(df, aes(x = sx, y = zobs)) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_minimal()
```

The data can be plotted with the trend removed. The `trend = "1st"` refers to a linear (first-order) trend.
```{r}
plot(gdf, trend = "1st")
```

Now the plots show the residuals from the first-order trend model.

As another example, consider the dataset called `topo` from the {MASS} package. The data are topographic heights (feet) within a 310 sq ft domain.
```{r}
library(MASS)

data(topo)
topo.gdf <- as.geodata(topo)
plot(topo.gdf)
```

Note the trend in the north-south direction and the skewness in the observed values. 

Examine the residuals after removing a first-order trend from the observations.
```{r}
plot(topo.gdf, trend = "1st")
```

The north-south trend is removed and the observations have a more symmetric distribution. There appears to be some non-linear trend in the east-west direction. 

Examine the residuals after removing a second-order trend.
```{r}
plot(topo.gdf, trend = "2nd")
```

The residuals from a second-order polynomial fit are symmetric and the trends are gone. However, the residuals appear to show spatial autocorrelations (areas with above and below residuals). 

### Empirical variograms

Consider the dataset `s100` available from the {geoR} package. The `points.geodata()` function produces a bubble plot showing the locations of the observations and the relative magnitude of the `z` variable.
```{r}
data(s100)
points.geodata(s100) 
```

The `variog()` function computes the empirical variogram. Here we save it in the object `s100.v`.
```{r}
s100.v <- variog(s100)
str(s100.v)
```

Information in the variogram object includes the lag distances (`u`), the values of the variogram at those distances (`v`), the number of distance pairs used to compute the variogram values at each lag (`n`), the standard deviation of the variogram values, and the coefficients of the trend surface (with no trend, the value is the overall mean of the data) (`beta.ols`) among other information.

Note: Mathematically lag distance is denoted with $h$ but in the {geoR} package it is `u`.

Verify the mean value.
```{r}
mean(s100$data)
```

Plot the variogram.
```{r}
plot(s100.v)
```

The semivariance ($\gamma(u)$) is plotted against lag distance ($u$). Values increase with increasing lag until a lag distance of about 1. 

At large lags there are fewer estimates so the values have greater variance. A model for the semivariance is fit only for the the increasing portion of the graph.

Another example: variogram of the `topo` dataset.
```{r}
topo.v <- variog(topo.gdf)
plot(topo.v, xlab = "Lagged Distance", 
     ylab = expression(paste(gamma, "(u) [ft", {}^2, "]")),
     las = 1, pch = 16)
```

The variogram values have units of square feet and are calculated using point pairs at lag distances within a lag tolerance. The number of point pairs depends on the lag so the variogram values are less precise at large distance.

Plot the number of point pairs used as a function of lag distance.
```{r}
ggplot(data.frame(u = topo.v$u, n = topo.v$n), aes(x = u, y = n)) +
  geom_point() +
  xlab("Lag Distance") + ylab("Number of Observation Pairs") +
  theme_minimal()
```

### Wolfcamp aquifer data

Some years ago there were three nuclear waste repository sites being proposed in Nevada, Texas, and Washington. The site needs to be larger enough for more than 68,000 high-level waste containers placed underground, about 9 m (~30 feet) apart, in trenches surrounded by salt. In July of 2002 the Congress approved [Yucca Mountain](https://en.wikipedia.org/wiki/Yucca_Mountain_nuclear_waste_repository), Nevada, as the nation’s first long-term geological repository for spent nuclear fuel and high-level radioactive waste.

The site must isolate the waste for 10,000 years. Leaks could occur, however, or radioactive heat could cause tiny quantities of water in the salt to migrate toward the heat until eventually each canister is surrounded by 22.5 liters of water (~6 gallons). A chemical reaction of salt and water can create hydrochloric acid that might corrode the canisters.

The piezometric-head data at the site were obtained by drilling a narrow pipe into the aquifer and letting water seeks its own level in the pipe (piezometer). 

The head measurements, given in units of feet above sea level, are from drill stem tests and indicate the total energy of the water in units of height. The higher the head height, the greater the potential energy. 

Water flows away from areas of high potential energy so aquifer discharge is proportional to the gradient of the piezometric head.

The data are in `wolfcamp.csv` on my website.

#### Step 1: Examine the observed data for trends, check for normality

Get the data.
```{r}
L <- "http://myweb.fsu.edu/jelsner/temp/data/wolfcamp.csv"
wca.df <- read.csv(L, header = TRUE)
```

Create a simple feature data frame and then map the locations and head heights.
```{r}
library(sf)
wca.sf <- st_as_sf(x = wca.df, 
                   coords = c("lon", "lat"),
                   crs = "+proj=longlat +datum=WGS84")

library(mapview)
mapView(wca.sf, 
        zcol = "head")
```

Convert the data frame to a `geodata` object. Note there is no method to do this from a simple feature.
```{r}
wca.gdf <- as.geodata(wca.df, 
                      coords.col = 1:2, 
                      data.col = 3)
```

Find the duplicate location(s).
```{r}
dup.coords(wca.gdf)
```

The locations are the same, but the data values are different. This may represent an error or multiple measurements at this one location.  

We can average the values or we can exclude a row. Here we remove the row 30 observation.
```{r}
wca.gdf <- as.geodata(wca.df[-30, ], 
                      coords.col = 1:2, 
                      data.col = 3)
summary(wca.gdf)
```

There are 84 well sites. The spatial bounding box is given under coordinates summary. The minimum distance between sites is .01 degrees and the maximum distance is 4.6 degrees.

The data values are summarized. The values are piezometric head heights in units of feet.

The `plot()` method for `geodata` provides a panel of plots with information about the data useful for modeling them.
```{r}
plot(wca.gdf)
```

The upper left panel is a graph of the observed locations with symbols reflecting quartiles of the observed piezometric head heights. There is a clear trend in the data with the highest potential energy over the southwest (red crosses) and lowest over the northeast (blue circles). 

The nature of this trend can be seen in the upper right and lower left panels. The upper right panel plots the data against the y coordinate and the lower left panel plots the data against the x coordinate. Both graphs indicate a linear trend.

A histogram of the head heights is shown in the lower right panel. The rug locates the values along the data axis. The data are bi-modal and skewed to the right.

Repeat the plot after removing the 1st order trend. What happens?
```{r}
plot(wca.gdf, trend = "1st")
```

With the trend removed the variation of values appears to be roughly symmetric and the high and low values are mixed. However, there is some spatial grouping to the residuals.

#### Step 2: Compute empirical variograms

We noted the maximum distance between any two locations is 4.6 degrees. It is a good idea to plot the variogram values for distances only between 0 and about 1/2 the maximum distance.

Since there is a linear trend in the data over the spatial domain it is removed (trend argument) before computing the variogram values.
```{r}
plot(variog(wca.gdf, 
            trend = "1st", 
            max.dist = 2.3))
```

Here we see an increase in the variance with distance until about one degree, then the values fluctuate about a variance of about 41000 (ft$^2$).

What does the variogram look like if we do not first remove the trend?

This continuously increasing set of variances with little fluctuation about a best fit curve indicates a trend in the data that must be removed before the variogram is modeled. 

There are two sources of variation in the field values: trend and spatial correlation. Trends are modeled with smooth curves and correlations are modeled with the variogram.

The `variog()` function has options for specifying a classical or modulus estimator. By default the function uses the classical estimator. To override this, include the `estimator.type = "modulus"` argument. The modulus estimator of the variogram is more resistant to outliers in the data.

Compare the classical with the modulus variograms for the Wolfcamp aquifer data.
```{r}
par(mfrow = c(1, 2))
plot(variog(wca.gdf, trend = "1st", 
            max.dist = 2.3), 
     main = "Classical")
plot(variog(wca.gdf, trend  ="1st", 
            max.dist = 2.3, 
            estimator.type = "modulus"),
     main = "Modulus")
```

The two variograms are nearly identical.

### Variogram cloud

We can examine the makeup of the variogram in more detail using a 'variogram cloud'. Consider again the simulated data set `s100`.
```{r}
par(mfrow = c(1, 1))
plot(s100)
```

There appears to be a trend in the east-west direction with higher values in the east. This is seen clearly in the bottom left panel.

Remove this first-order trend.
```{r}
plot(s100, 
     trend = "1st")
```

We can see that the trend is adequately modeled with a 1st-order surface. Residuals appear symmetric about zero. 

A summary of the data indicates the maximum lagged distance of 1.3 so the `max.dist` argument is set to .6. 
```{r}
summary(s100)
```

Here we specify the eleven variogram estimates between 0 and .6 using the `uvec` argument.
```{r}
plot(variog(s100, 
            trend = "1st", 
            uvec = 11, 
            max.dist = .6))
```

We 'decompose' the variogram estimates with the `option = "cloud"`.
```{r}
c1 <- variog(s100, trend = "1st", 
             option = "cloud", 
             max.dist = .6)
plot(c1)
```

Each point on the plot is the difference (absolute value) between two observed values (y-axis) a lag distance apart (x-axis). There are 100 observations so there are 100 * 99/2 = 4950 possible points on this plot. Actually here only 3131 since we limit the distance to less than .6 degrees.

There is a tendency for more large differences as the lag increases but most differences are small for a given lag. The variability in observational differences is quite large especially for longer lag distances.

The variogram computes the sum of the squared differences as a function of lag distance grouped by a lag tolerance. Here we indirectly specify the lag tolerance with the `uvec` argument and compare the variogram with the variogram cloud.
```{r}
par(mfrow = c(1, 2))
v1 <- variog(s100, trend = "1st", 
            uvec = seq(0, .6, l = 11))
layout(matrix(c(1, 2), byrow = TRUE, ncol = 2), 
       respect = TRUE)
plot(v1); plot(c1)
```

Note the difference in the scale on the y-axis between the variogram cloud and the variogram is a factor of 10.

The variogram cloud can be grouped into lag distance classes (bins) and displayed with a box plot. This gives an idea of the general shape the variogram model should take.
```{r}
par(mfrow = c(1, 1))
b1 <- variog(s100, trend = "1st", 
            max.dist = .6, 
            bin.cloud = TRUE, 
            estimator.type = "classical")
plot(b1, bin.cloud = TRUE)
```

The box plot summarizes the squared differences for a given lag (and lag tolerance) with the median. There is a box plot for each lag but the lag distance is given as a bin number. 

This summary information helps us anticipate the type of variogram model. As an aside, note how to use the expression and paste functions to include symbols as part of the axis label.
```{r}
df <- data.frame(u = b1$u, v = b1$v)
ggplot(df, aes(u, v)) + 
  geom_point() + 
  geom_smooth(span = .8) +
  scale_y_continuous(limits = c(0, .6)) +
  ylab(expression(paste("Variogram [", gamma,"(h)]"))) +
  xlab("Lagged distance (h)")
```

The blue line is a least-squares regression smoother through the variogram estimates. The fact that it is not horizontal indicates spatial autocorrelation in the values that is separate from the first-order trend. The shape of the blue line gives an idea of the type of variogram family of models we should consider.

Now we can guess at a family for the variogram model and eyeball the parameters. Recall the plot from Lesson 18. This is called variography. We will discuss this in Lesson 20.

```{r}
plot(variog(s100, max.dist = 1), 
     xlab = "Lagged Distance (h)",
     ylab = expression(paste(gamma,"(h)")), 
     las = 1, pch = 16)
abline(h = 0)
abline(h = .15, col = "red")
arrows(0, 0, x1 = 0, y1 = .15, length = .1)
text(0, y = .05, labels = "nugget", pos=4)
abline(h = .9, col = "red")
arrows(0, .15, x1 = 0, y1 = .9, length = .1)
text(0, y = .8, labels="sill (partial sill)", pos=4)
abline(v = .6, col = "red")
arrows(0, 1, x1 = .6, y1 = 1, length = .1)
text(.4, y = 1.04, labels = "range")
```

* Lag (lag distance): Relative distance between observation locations.
* Nugget (nugget, nugget variance, or nugget effect): The height of the variogram at zero lag. The nugget is the variation in the values at the observation locations without regard to spatial variation. Related to the observation (or measurement) precision.
* Sill: The height of the variogram at which the values are uncorrelated.
* Relative nugget effect: The ratio of the nugget to the sill expressed as a percentage.
* Range: The distance beyond which the values are uncorrelated. The range is indicated on the empirical variogram as the position along the horizontal axis where values of the variogram reach a constant height.
